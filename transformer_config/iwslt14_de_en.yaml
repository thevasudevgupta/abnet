# IWSLT14 De-En
encoder_id: bert-base-german-cased
decoder_id: bert-base-uncased
enc_ffn_adapter_requires_grad: True
dec_ffn_adapter_requires_grad: True
cross_attn_adapter_requires_grad: True
length_embed_requires_grad: True
# its fine to overlap with pad token since embedding layer is different in both cases
length_token: 0
num_lengths: 48
enc_ffn_adapter_config:
  hidden_size: 768
  intermediate_size: 512
  layer_norm_eps: 1.e-12
dec_ffn_adapter_config: 
  hidden_size: 768
  intermediate_size: 2048
  layer_norm_eps: 1.e-12
cross_attn_adapter_config: 
  hidden_size: 768
  layer_norm_eps: 1.e-12
  hidden_dropout_prob: 0.1
  num_attention_heads: 12
  attention_probs_dropout_prob: 0.1
